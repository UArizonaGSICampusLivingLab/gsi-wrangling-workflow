---
title: "Green Stormwater Infrastructure Data Wrangling"
author: "Eric R. Scott"
date: "`r Sys.Date()`"
output: html_document
resource_files:
- renv.lock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
zentracloud::setZentracloudOptions(
  token = Sys.getenv("ZENTRACLOUD_TOKEN"),
  domain = "default"
)

library(boxr)
library(jose)
library(tidyverse)
library(zentracloud)
library(httr2)
library(jsonlite)
source("R/gsi_get_data.R")
source("R/gsi_get_eto.R")
```

<!--
## First time publishing

Publishing this workflow to Posit Connect will fail the first time because the environment variable `BOX_TOKEN_TEXT` is unset.  But, you still can find the app on viz.datascience.arizona.edu and add that env var.  Then, re-publish, and it should work.
-->
## Box authorization

```{r auth}
box_auth_service(token_text = Sys.getenv("BOX_TOKEN_TEXT"))
# box_auth() ## If you want to run locally use this instead of box_auth_service()
dir_id <- "233031886906"
box_setwd(dir_id)
#list current files
files <- box_ls() |> as_tibble()
```

## Wrangling workflow

### Hourly data

```{r update_data}
#does gsi_living_lab_data.csv exist already?
if ("gsi_living_lab_data.csv" %in% files$name) {
  old_dat <- files |> 
    filter(name == "gsi_living_lab_data.csv") |> 
    pull(id) |> 
    box_read(read_fun = readr::read_csv)
  prev_end <- max(old_dat$datetime) |> with_tz("America/Phoenix")
} else {
  old_dat <- tibble()
  prev_end <- ymd("2023-06-05", tz = "America/Phoenix") #first date of data
}

# Get the data and wrangle it

new_dat <- gsi_get_data(start = prev_end)

# bind old to new
all_dat <- 
  bind_rows(old_dat, new_dat) |> 
  filter(!is.na(sensor)) |> # this is somewhat redundant, but having this in gsi_get_data() only removes these from new data
  distinct() |> #remove duplicate rows
  arrange(datetime) 
```

Missing data due to a device being offline is only indicated by missing datetimes. This next chunk makes missing datetimes explicit.

```{r}
all_dat_final <-
  all_dat |> 
  complete(
    nesting(device_sn, sensor, port),
    datetime = seq(min(datetime), max(datetime), by = "hour")
  )
```

### ETo

Get site_info.csv from box

```{r}
site_info <- 
  files |> 
  filter(name == "site_info.csv") |> 
  pull(id) |> 
  box_read()

inputs <-
  site_info |> 
  filter(sensor_model == "ATMOS41") |> 
  select(device_sn, port_num = port, wind_height = depth_height_m, elevation = Elevation, latitude = Latitude)
```

Use site info to get most recent ETo values for all locations from API

```{r}
eto_list <- pmap(inputs, gsi_get_eto)
eto_new <- list_rbind(eto_list)
```

Read in previously saved ETo data and join.  Joining rather than binding rows here because there will be a lot of overlap and this saves the trouble of removing duplicate dates. If column names change (either manually editing the file on Box or changes to `gsi_get_eto()`), this will break or create confusing column names.  I've made this chunk will error if the colnames aren't what are expected because dealing with a change like this will require some manual finessing to harmonize the old and new data.

```{r}
eto_prev <- 
  files |> 
  filter(name == "gsi_living_lab_ETo.csv") |> 
  pull(id) |> 
  box_read(read_fun = readr::read_csv)

# left_join(eto_prev, eto_new) 
# just checking that old values didn't change.  If they did, then the join_by()
# wouldn't include ETo.value and the joined dataframe wouldn't have the same
# number of rows/columns as the original

eto_full <-
  full_join(eto_prev, eto_new) |> 
  arrange(datetime, device_sn)

stopifnot(colnames(eto_full) == c("device_sn", "port", "datetime", "ETo.value", "ETo.error_flag", "ETo.error_description", "ETo.units"))
```


## Write data to box

```{r}
box_write(all_dat_final, "gsi_living_lab_data.csv", write_fun = readr::write_csv)
box_write(eto_full, "gsi_living_lab_ETo.csv", write_fun = readr::write_csv)
```

