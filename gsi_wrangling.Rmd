---
title: "Green Stormwater Infrastructure Data Wrangling"
author: "Eric R. Scott"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
resource_files:
- renv.lock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
zentracloud::setZentracloudOptions(
  token = Sys.getenv("ZENTRACLOUD_TOKEN"),
  domain = "default"
)

library(boxr)
library(jose)
# library(tidyverse) #don't actually need *all* of tidyverse
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(readr)
library(ggplot2)

library(fs)
library(units)
library(withr)
library(lubridate)
library(zentracloud)
library(httr2)
library(jsonlite)
library(glue)
source("R/gsi_get_data.R")
source("R/gsi_get_eto.R")
source("R/calc_hi.R")
source("R/calc_wind_chill.R")
```

<!--
## First time publishing

Publishing this workflow to Posit Connect will fail the first time because the environment variable `BOX_TOKEN_TEXT` is unset.  But, you still can find the app on viz.datascience.arizona.edu and add that env var.  Then, re-publish, and it should work.
-->
## Box authorization

```{r auth}
box_auth_service(token_text = Sys.getenv("BOX_TOKEN_TEXT"))
# box_auth() ## If you want to run locally use this instead of box_auth_service()
dir_id <- "250527085917"
box_setwd(dir_id)
#list current files
files <- box_ls() |> as_tibble()
```

## Wrangling workflow

### Hourly data

Retrieve the data already on box

```{r old_data}
#does gsi_living_lab_data.csv exist already?
if ("gsi_living_lab_data.csv" %in% files$name) {
  old_dat <- files |> 
    filter(name == "gsi_living_lab_data.csv") |> 
    pull(id) |> 
    box_read(read_fun = readr::read_csv)
  prev_end <- max(old_dat$datetime) |> with_tz("America/Phoenix")
  #allow for a week of overlap in case values got updated on the API.  Coalescing join below will take care of duplicates.
  prev_end <- prev_end - weeks(1)
} else {
  old_dat <- tibble()
  prev_end <- ymd("2023-06-05", tz = "America/Phoenix") #first date of data
}
```

Retrieve new data

**Note: if you need to manually run this to retrieve previous data from the API in the event that a sensor has been down, just manually assign a datetime to `start=` in the chunk below.**

```{r update_data, results='hide', message=FALSE}
# Get the data and wrangle it
new_dat <-
  gsi_get_data(start = prev_end)
```

#### Add calculated variables

Add columns for adjusted temperature (wind chill and heat index) and plant available water.

```{r calc_vars}
new_dat <- 
  new_dat |>
  mutate(
    air_temperature_adj.value = 
      calc_hi(air_temperature.value, relative_humidity.value) |>
      calc_wind_chill(wind_speed.value) |>
      round(digits = 1),
    paw.value = (water_content.value - 0.06) / 0.11 * 100
)
```

#### Join to existing data

This is a bit tricky and there is possibly an easier to understand solution, but I believe this works for now.
First, a full join by only the "keys" `device_sn`, `sensor`, `port`, and `datetime` combines the old and new data while creating duplicate colums for every variable, suffixed `_old` and `_new`.
Then, for each variable name, `coalesce()` is used to combine old and new, updating `NA`s in the old data with new values.

```{r}
#Join by keys
all_dat <-
  full_join(
    old_dat,
    new_dat,
    by = join_by(device_sn, sensor, port, datetime),
    suffix = c("_old", "_new")
  )
vars_old <- colnames(all_dat)[str_ends(colnames(all_dat), "_old")]
vars_new <- colnames(all_dat)[str_ends(colnames(all_dat), "_new")]
#coalesce old and new columns for each original variable column
purrr::walk2(vars_old, vars_new, function(old, new) {
  # column names have to be provided as symbols, not character values
  to_merge <- rlang::syms(c(old, new))
  orig <- str_remove(old, "_old$")
  # `<<-` is needed to actually update the dataframe, not just spit out a list of new dataframes
  all_dat <<- all_dat |> 
    # {{}}, :=, !!! are all operators from `rlang`
    mutate({{orig}} := coalesce(!!!to_merge), .keep = "unused")
}) 

all_dat <- all_dat |> 
  filter(!is.na(sensor)) |> # this is somewhat redundant, but having this in gsi_get_data() only removes these from new data
  #remove any duplicate rows that creeped in
  distinct() |> 
  arrange(datetime)
```

Missing data due to a device being offline is only indicated by missing datetimes.
This next chunk makes missing datetimes explicit.

```{r explicit_na}
all_dat_final <-
  all_dat |> 
  complete(
    nesting(device_sn, sensor, port),
    datetime = seq(min(datetime), max(datetime), by = "hour")
  ) |> 
  distinct() #remove duplicate rows again, just in case.
```

### ETo

Get `site_info.csv` from box

```{r eto_siteinfo}
site_info <- 
  files |> 
  filter(name == "site_info.csv") |> 
  pull(id) |> 
  box_read()

inputs <-
  site_info |>
  filter(sensor_model == "ATMOS41") |>
  select(
    device_sn,
    port_num = port,
    wind_height = depth_height_m,
    elevation = Elevation,
    latitude = Latitude
  )
```

Use site info to get most recent ETo values for all locations from the Zentra Cloud API

```{r eto_get, results='hide', message=FALSE}
eto_list <- pmap(inputs, gsi_get_eto)
eto_new <- list_rbind(eto_list)
```

Zentra Cloud appears to calculate daily ETo even with partial hourly data for a day, so the oldest date of `eto_new` is not reliable and gets removed.

```{r eto_new}
eto_new <- 
  eto_new |> 
  filter(datetime != min(datetime, na.rm = TRUE))
```

Read in previously saved ETo data and join.

```{r eto_prev}
if ("gsi_living_lab_ETo.csv" %in% files$name) {
  eto_prev <- 
    files |> 
    filter(name == "gsi_living_lab_ETo.csv") |> 
    pull(id) |> 
    box_read(read_fun = readr::read_csv)
} else {
  eto_prev <- tibble()
}
```

Zentracloud calculates daily ETo even with partial hourly data, so if data is uploaded retroactively, ETo values may update.
If that is the case, we keep only the new, updated values.

```{r eto_update}
eto_updates <- 
  full_join(
    eto_prev |> select(device_sn, port, datetime, ETo.value),
    eto_new |> select(device_sn, port, datetime, ETo.value),
    by = join_by(device_sn, port, datetime),
    suffix = c("_old", "")
  )
```


```{r eto_diffs}
eto_diffs <- eto_updates |> 
  filter(ETo.value != ETo.value_old)
if (nrow(eto_diffs) > 0) {
  warning("Some values of ETo have changed on Zentra Cloud---updating the following values:")
  eto_diffs
}
```

```{r}
#Overwrite old values with new ones (unless they are old enough to be missing from the new values)
eto_full <- eto_updates |> 
  mutate(ETo.value = ifelse(is.na(ETo.value), ETo.value_old, ETo.value)) |> 
  select(-ETo.value_old)
```

## Write data to box

```{r write_box}
box_write(all_dat_final, "gsi_living_lab_data.csv", write_fun = readr::write_csv)
box_write(eto_full, "gsi_living_lab_ETo.csv", write_fun = readr::write_csv)
```

